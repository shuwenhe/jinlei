import os
# ç§»é™¤äº† DirectoryLoaderï¼Œå› ä¸ºå®ƒä¸å…¼å®¹ loader_map å‚æ•°
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader
# å¯¼å…¥è·¯å¾„å·²ä¿®å¤
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

# --- é…ç½®å‚æ•° ---
KNOWLEDGE_DIR = "./knowledge_base"
FAISS_INDEX_PATH = "./faiss_jinlei_index"
OLLAMA_EMBEDDING_MODEL = "bge-m3" # è¯·ç¡®ä¿å·²é€šè¿‡ ollama pull m3e-base æ‹‰å–

def build_index():
    """
    åŠ è½½ã€åˆ‡åˆ†æ–‡æ¡£ï¼Œåˆ›å»ºå¹¶ä¿å­˜ FAISS å‘é‡çŸ¥è¯†åº“ã€‚
    ä½¿ç”¨æ‰‹åŠ¨éå†ä»£æ›¿ DirectoryLoader(loader_map)ï¼Œå¹¶å¢åŠ åˆ‡åˆ†å®¹é”™é€»è¾‘ã€‚
    """
    # 1. æ–‡æ¡£åŠ è½½
    print(f"--- 1. æ­£åœ¨åŠ è½½ {KNOWLEDGE_DIR} ä¸­çš„æ–‡æ¡£... ---")
    
    # å®šä¹‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹å’Œå¯¹åº”çš„åŠ è½½å™¨
    LOADER_MAPPING = {
        ".pdf": PyPDFLoader,
        ".docx": UnstructuredWordDocumentLoader,
    }
    
    documents = []
    
    # æ‰‹åŠ¨éå†çŸ¥è¯†åº“ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ (åŒ…æ‹¬å­ç›®å½•)
    for root, _, files in os.walk(KNOWLEDGE_DIR):
        for file in files:
            file_path = os.path.join(root, file)
            ext = os.path.splitext(file)[1].lower()
            
            if ext in LOADER_MAPPING:
                LoaderClass = LOADER_MAPPING[ext]
                print(f"   -> æ­£åœ¨åŠ è½½æ–‡ä»¶: {file}")
                try:
                    loader = LoaderClass(file_path)
                    documents.extend(loader.load())
                except Exception as e:
                    print(f"âŒ æ–‡ä»¶ {file} åŠ è½½å¤±è´¥: {e}")

    if not documents:
        print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•æ”¯æŒæ ¼å¼çš„æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶å¤¹ã€‚")
        return
        
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£é¡µé¢/å—ã€‚")

    # 2. æ–‡æœ¬åˆ‡åˆ† (æ ¸å¿ƒä¿®å¤åŒºåŸŸ)
    print("--- 2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†... ---")
    
    # ğŸŒŸ ä¼˜åŒ–ç‚¹ 1: æ‰“å°æ–‡æ¡£å†…å®¹çš„é•¿åº¦ï¼Œä»¥ä¾¿è¯Šæ–­
    total_chars = sum(len(doc.page_content) for doc in documents)
    print(f"â­ å¾…åˆ‡åˆ†æ–‡æ¡£æ€»å­—ç¬¦æ•°ï¼š{total_chars}")
    
    # ğŸŒŸ ä¼˜åŒ–ç‚¹ 2: é¦–æ¬¡å°è¯•åˆ‡åˆ†
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""], 
        length_function=len # æ˜ç¡®ä½¿ç”¨æ ‡å‡†çš„ Python len() å‡½æ•°
    )
    texts = text_splitter.split_documents(documents)
    
    # ğŸŒŸ ä¼˜åŒ–ç‚¹ 3: å¦‚æœåˆ‡åˆ†ç»“æœä¸º 0ï¼Œå°è¯•ä½¿ç”¨è¾ƒå°çš„ chunk_size è¿›è¡Œå®¹é”™
    if not texts:
        print("ğŸ’¡ ç¬¬ä¸€æ¬¡åˆ‡åˆ†ç»“æœä¸º 0ã€‚å¯èƒ½å†…å®¹å¤ªçŸ­ï¼Œå°è¯•ä½¿ç”¨è¾ƒå°çš„ chunk_size (ä¾‹å¦‚ 400)...")
        text_splitter_small = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""], 
            length_function=len
        )
        texts = text_splitter_small.split_documents(documents)
    
    if not texts:
        print("âŒ ä¸¥é‡è­¦å‘Šï¼šæ–‡æœ¬åˆ‡åˆ†ç»“æœä»ä¸º 0ã€‚è¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–ä¸å¯æå–ã€‚")
        return # æå‰é€€å‡ºï¼Œé¿å… FAISS é”™è¯¯
        
    print(f"âœ… æ–‡æ¡£åˆ‡åˆ†æˆ {len(texts)} ä¸ªçŸ¥è¯†å—ã€‚")

    # 3. å‘é‡åŒ–ä¸çŸ¥è¯†åº“æ„å»º
    print(f"--- 3. æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹ ({OLLAMA_EMBEDDING_MODEL}) å¹¶æ„å»º FAISS ç´¢å¼•... ---")
    
    embeddings = OllamaEmbeddings(
        model=OLLAMA_EMBEDDING_MODEL,
        base_url="http://localhost:11434" # ç¡®ä¿ Ollama æœåŠ¡åœ°å€æ­£ç¡®
    )
    
    try:
        # FAISS éœ€è¦éç©ºåˆ—è¡¨
        db = FAISS.from_documents(texts, embeddings)
        
        # 4. ä¿å­˜ FAISS ç´¢å¼•
        db.save_local(FAISS_INDEX_PATH)
        print(f"âœ… FAISS ç´¢å¼•å·²æˆåŠŸä¿å­˜åˆ°: {FAISS_INDEX_PATH}")
        print("\nğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œ Web åº”ç”¨äº†ã€‚")
    except Exception as e:
        # å¦‚æœä¸æ˜¯ list index out of rangeï¼Œæ‰“å°å…·ä½“é”™è¯¯
        print(f"âŒ å‘é‡åŒ–æˆ– FAISS çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")


if __name__ == "__main__":
    if not os.path.exists(KNOWLEDGE_DIR):
        print(f"åˆ›å»ºçŸ¥è¯†åº“ç›®å½•: {KNOWLEDGE_DIR}")
        os.makedirs(KNOWLEDGE_DIR)
        print("è¯·å°† PDF/Word æ–‡æ¡£æ”¾å…¥æ­¤ç›®å½•åï¼Œé‡æ–°è¿è¡Œæœ¬è„šæœ¬ã€‚")
    else:
        build_index()

# è¿è¡Œå‘½ä»¤: python build_knowledge_base.py
