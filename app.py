import streamlit as st
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import os

# --- é…ç½®å‚æ•° (éœ€ä¸ build_knowledge_base.py ä¿æŒä¸€è‡´) ---
FAISS_INDEX_PATH = "./faiss_jinlei_index"
OLLAMA_LLM_MODEL = "qwen:7b" # ç¡®ä¿å·²æ‹‰å– Qwen æ¨¡å‹
OLLAMA_EMBEDDING_MODEL = "m3e-base" 

# --- åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ (ä½¿ç”¨ st.cache_resource é¿å…é‡å¤åŠ è½½) ---

@st.cache_resource
def load_rag_chain():
    """åŠ è½½ LLMã€Embedding æ¨¡å‹ã€FAISS ç´¢å¼•å¹¶åˆ›å»º RAG é—®ç­”é“¾ã€‚"""
    try:
        # 1. åˆå§‹åŒ– Embedding
        embeddings = OllamaEmbeddings(
            model=OLLAMA_EMBEDDING_MODEL,
            base_url="http://localhost:11434"
        )
        
        # 2. åŠ è½½ FAISS çŸ¥è¯†åº“
        if not os.path.exists(FAISS_INDEX_PATH):
            st.error(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° FAISS ç´¢å¼•ç›®å½• '{FAISS_INDEX_PATH}'ã€‚")
            st.error("è¯·å…ˆè¿è¡Œ 'python build_knowledge_base.py' æ„å»ºçŸ¥è¯†åº“ï¼")
            return None

        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        retriever = db.as_retriever(search_kwargs={"k": 3}) # æ£€ç´¢æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£å—

        # 3. åˆå§‹åŒ– LLM
        llm = Ollama(model=OLLAMA_LLM_MODEL, temperature=0.1, base_url="http://localhost:11434")

        # 4. å®šåˆ¶ Prompt æ¨¡æ¿ (ä¼˜åŒ–å›ç­”ç»“æ„å’Œè§’è‰²å®šä½)
        template = """
        ä½ æ˜¯ä¸€åèµ„æ·±çš„**é‡‘é›·ç§‘æŠ€**ç»´ä¿®å·¥ç¨‹å¸ˆã€‚
        è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„**å‚è€ƒç»´ä¿®æ–‡æ¡£ç‰‡æ®µ**ï¼Œç»™å‡ºä¸“ä¸šã€æ¸…æ™°ã€åˆ†æ­¥çš„ç»´ä¿®å»ºè®®ã€‚
        
        **å›ç­”è¦æ±‚å’Œç»“æ„ï¼š**
        1. **æ•…éšœè¯Šæ–­:** ç®€è¦æ€»ç»“ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ•…éšœç‚¹ã€‚
        2. **ç»´ä¿®å»ºè®®/æ­¥éª¤:** åˆ—å‡ºå…·ä½“çš„ã€å¯æ“ä½œçš„**åˆ†æ­¥**è§£å†³æ–¹æ¡ˆã€‚
        3. **å‚è€ƒä¾æ®:** æŒ‡å‡ºå»ºè®®æ˜¯åŸºäºå“ªäº›æ–‡æ¡£ä¿¡æ¯å¾—å‡ºçš„ã€‚
        
        **ã€å‚è€ƒç»´ä¿®æ–‡æ¡£ç‰‡æ®µã€‘**
        {context}
        
        **ã€ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‘**
        {question}
        
        **ã€é‡‘é›·ç§‘æŠ€ç»´ä¿®å»ºè®®ã€‘**
        """
        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

        # 5. åˆ›å»º RAG é“¾
        document_chain = create_stuff_documents_chain(llm, QA_CHAIN_PROMPT)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        
        st.success("âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
        return retrieval_chain

    except Exception as e:
        st.error(f"âŒ RAG ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œï¼Œæˆ–æ¨¡å‹æ˜¯å¦æ‹‰å–: {e}")
        return None

# --- Streamlit Web ç•Œé¢ ---

st.set_page_config(page_title="é‡‘é›·ç§‘æŠ€æ™ºèƒ½ç»´ä¿®é—®ç­”ç³»ç»Ÿ", layout="wide")
st.title("âš¡ é‡‘é›·ç§‘æŠ€å¤§æ¨¡å‹ç»´ä¿®çŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
st.caption(f"ç”± Ollama ({OLLAMA_LLM_MODEL} + {OLLAMA_EMBEDDING_MODEL}) & LangChain æä¾›æŠ€æœ¯æ”¯æŒ")

# å°è¯•åŠ è½½ RAG é“¾
rag_chain = load_rag_chain()

if rag_chain:
    # åˆå§‹åŒ–å†å²èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # å±•ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨é‡åˆ°çš„ç»´ä¿®é—®é¢˜ï¼Œä¾‹å¦‚ï¼š'è®¾å¤‡è¿è¡Œæ—¶ï¼ŒæŒ‡ç¤ºç¯é—ªçƒä½†æ— æ³•å¯åŠ¨ï¼Œåº”è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ'"):
        # å­˜å‚¨å¹¶æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
            
        # è°ƒç”¨ RAG é“¾
        with st.spinner("ğŸ”§ æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“å¹¶ç”Ÿæˆä¸“ä¸šç»´ä¿®å»ºè®®..."):
            try:
                # è°ƒç”¨ RAG é“¾è¿›è¡Œé—®ç­”
                response = rag_chain.invoke({"input": prompt}) 
                
                # LLM ç”Ÿæˆçš„æœ€ç»ˆå›ç­”
                assistant_response = response['answer']
                
                # æ£€ç´¢åˆ°çš„æºæ–‡æ¡£ä¿¡æ¯
                source_docs = response['context']
                
            except Exception as e:
                assistant_response = f"æŠ±æ­‰ï¼Œç³»ç»Ÿåœ¨å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"
                source_docs = []

            # æ˜¾ç¤ºæ¨¡å‹å›ç­”
            with st.chat_message("assistant"):
                st.markdown(assistant_response)
                
                # æ·»åŠ æ–‡æ¡£å¼•ç”¨ï¼Œå®ç°å¯æº¯æºæ€§ (åŠŸèƒ½ 4 çš„ä¼˜åŒ–)
                if source_docs:
                    with st.expander("ğŸ“š æŸ¥çœ‹å‚è€ƒæ–‡æ¡£å¼•ç”¨"):
                        st.markdown(f"**æ€»å…±æ£€ç´¢åˆ° {len(source_docs)} æ¡ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚**")
                        for i, doc in enumerate(source_docs):
                            source_name = doc.metadata.get('source', 'æœªçŸ¥æ–‡æ¡£')
                            st.subheader(f"ç‰‡æ®µ {i+1}ï¼šæ¥è‡ª {os.path.basename(source_name)}")
                            st.code(doc.page_content[:500] + "...", language='text') # åªæ˜¾ç¤ºå‰ 500 å­—ç¬¦

            # å­˜å‚¨æ¨¡å‹æ¶ˆæ¯
            st.session_state.messages.append({"role": "assistant", "content": assistant_response})
            
# è¿è¡Œå‘½ä»¤: streamlit run app.py
